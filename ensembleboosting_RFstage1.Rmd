
### code Read and partition data   
```{r message=FALSE, warning=FALSE, width = 200}
setwd("Z:/Cristina/MassNonmass/Section1 - ExperimentsUpToDate/experimentsRadiologypaper-revision/Tree-based-RF/ensemble-Treebased-RF")
library("RSQLite")

rpart_inputdata <- function(subdata) {
  sqlite <- dbDriver("SQLite")
  conn <- dbConnect(sqlite, "stage1localData.db")
  
  # 2) all T1W features
  lesionsQuery <- dbGetQuery(conn, "SELECT *
           FROM  stage1features
           INNER JOIN lesion ON (stage1features.lesion_id = lesion.lesion_id)
           INNER JOIN f_dynamic ON (stage1features.lesion_id = f_dynamic.lesion_id)
           INNER JOIN f_morphology ON (stage1features.lesion_id = f_morphology.lesion_id)
           INNER JOIN f_texture ON (stage1features.lesion_id = f_texture.lesion_id)")
  
  # prune entries and extract feature subsets
  # corresponds to 5 entries lesion info, 34 dynamic, 19 morpho, 34 texture fueatures
  lesionfields =  names(lesionsQuery)
  lesioninfo = lesionsQuery[c(1,2,150,151)]
  stage1features = lesionsQuery[c(3:103,124:127)]
  dynfeatures = lesionsQuery[c(154:187)]
  morphofeatures = lesionsQuery[c(190:208)]
  texfeatures = lesionsQuery[c(211:234)]
  
  # combine all features
  allfeatures = cbind(lesioninfo[c(2,3)], stage1features, dynfeatures, morphofeatures, texfeatures)   
  
  if(subdata=="mass"){
    # organized the data by subdata
    M<-subset(allfeatures, lesion_label=="massB" | lesion_label=="massM")
    ifelse( M$lesion_label == "massB", "NC", "C") -> M$lesion_label
    allfeatures = M
  }
  if(subdata=="nonmass"){
    # organized the data by subdata
    N<-subset(allfeatures, lesion_label=="nonmassB" | lesion_label=="nonmassM")
    ifelse( N$lesion_label == "nonmassB", "NC", "C") -> N$lesion_label
    allfeatures = N
  }
  if(subdata=="stage1"){
    # organized the data by subdata
    M<-subset(allfeatures, lesion_label=="massB" | lesion_label=="massM")
    ifelse( M$lesion_label == "massB", "mass", "mass") -> M$lesion_label
    N<-subset(allfeatures, lesion_label=="nonmassB" | lesion_label=="nonmassM")
    ifelse( N$lesion_label == "nonmassB", "nonmass", "nonmass") -> N$lesion_label
    allfeatures = data.frame(rbind(M,N)) 
  }
  if(subdata=="oneshot"){
    # organized the data by subdata
    M<-subset(allfeatures, lesion_label=="massB" | lesion_label=="massM")
    ifelse( M$lesion_label == "massB", "NC", "C") -> M$lesion_label
    N<-subset(allfeatures, lesion_label=="nonmassB" | lesion_label=="nonmassM")
    ifelse( N$lesion_label == "nonmassB", "NC", "C") -> N$lesion_label
    allfeatures = data.frame(rbind(M,N)) 
  }
  # procees data
  allfeatures$lesion_label <- as.factor(allfeatures$lesion_label)
  allfeatures$peakCr_inside <- as.factor(allfeatures$peakCr_inside)
  allfeatures$peakVr_inside <- as.factor(allfeatures$peakVr_inside)
  allfeatures$peakCr_countor <- as.factor(allfeatures$peakCr_countor)
  allfeatures$peakVr_countor <- as.factor(allfeatures$peakVr_countor)
  allfeatures$k_Max_Margin_Grad <- as.factor(allfeatures$k_Max_Margin_Grad)
  allfeatures$max_RGH_mean_k <- as.factor(allfeatures$max_RGH_mean_k)
  allfeatures$max_RGH_var_k <- as.factor(allfeatures$max_RGH_var_k)
  
  output <- allfeatures
  return(output)
}

```

### code to create a cross-validation set up: 
### cvfoldk = number of cv folds typically 5 or 10
### out: particvfoldK = all cv-K ids
```{r}
library(MASS)
library(caret)

cvfold_partition <- function(dat, cvfoldK){
  ndat = nrow(dat)
  outcomesetDi  <- dat$lesion_label
  #For multiple k-fold cross-validation, completely independent folds are created.
  #when y is a factor in an attempt to balance the class distributions within the splits.
  #The names of the list objects will denote the fold membership using the pattern 
  #"Foldi.Repj" meaning the ith section (of k) of the jth cross-validation set (of times).
  partitionsetDi <- createFolds(y = outcomesetDi, ## the outcome data are needed
                                k = cvfoldK, ## The percentage of data in the training set
                                list = TRUE) ## The format of the results. 
  return(partitionsetDi)
}
```


### code to sample kparti from a cross-validation set up: 
### kparti = k fold to exclude
### outs: cvTrainsetD, cvTestsetD
```{r}
kparti_sample <- function(dat, particvfoldK, cvfoldK, kparti){
  allparti = 1:cvfoldK
  allbutkparti = allparti[-kparti]
  cvfoldadd = c()
  for(i in 1:length(allbutkparti)){
    kadd = allbutkparti[i]
    cvfoldadd = c(cvfoldadd, particvfoldK[[kadd]])
  }
  # partition data
  cvTrainsetD <-  dat[ cvfoldadd ,]
  cvTestsetD <-   dat[-cvfoldadd ,]
  
  output <- list(cvTrainsetD=cvTrainsetD, cvTestsetD=cvTestsetD)
  return(output)
}

```


### code Feature selection: 
### Boruta, cvfold, 
```{r}
library(Boruta)
require(data.table)
require(ggplot2)

# function to produce correlation coefficients on pair plots
panel.cor <- function(x, y, digits = 2, cex.cor, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  # correlation coefficient
  r <- cor(x, y)
  txt <- format(c(r, 0.123456789), digits = digits)[1]
  txt <- paste("r= ", txt, sep = "")
  text(0.5, 0.6, txt)
  
  # p-value calculation
  p <- cor.test(x, y)$p.value
  txt2 <- format(c(p, 0.123456789), digits = digits)[1]
  txt2 <- paste("p= ", txt2, sep = "")
  if(p<0.01) txt2 <- paste("p= ", "<0.01", sep = "")
  text(0.5, 0.4, txt2)
}

subset_select <- function(setTrain){
  featsel_boruta <-Boruta(lesion_label ~., data=setTrain[,2:ncol(setTrain)], doTrace=2, ntree=1000)
  print(featsel_boruta)
  plot(featsel_boruta)
  
  relevant <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Confirmed"]
  relevant_features = setTrain[c(names(relevant))]
  tentative <- featsel_boruta$finalDecision[featsel_boruta$finalDecision == "Tentative"]
  tentative_features = setTrain[c(names(tentative))]
  sel_features = cbind(setTrain[c(1,2)], relevant_features, tentative_features)
  
  super.sym <- trellis.par.get("superpose.symbol")
  ## pair plots for reatures
  setTrainrelevant = setTrain[c(names(relevant))]
  pairs(relevant_features, 
        upper.panel = panel.cor,
        pch = super.sym$pch[1:2],
        col = super.sym$col[1:2],
        text = list(levels(setTrainrelevant$lesion_label)),
        main = "Relevant features")
  
  return(sel_features)
}

```


### code forest Train: 
### parameters, T= # of trees, D= tree depth, dat
```{r}
library(klaR)
library(rpart)
library(rpart.plot)
library(ada)

# bagged training was introduced as a way of reducing possible overfitting and 
# improving the generalization capabilities of random forests. 
# The idea is to train each tree in a forest on a different training subset, sampled at random from the same labeled database. 
rpart_adaforestTrain <- function(T, D, dat, dattest) {
  # set control
  adacontrol <- rpart.control(cp = -1, minsplit = 0, xval = 0, maxdepth = D)

  # init forest
  forest = list()
  for (t in 1:T){
    #cat("Tree # ", t, "\n")  
    
    # build bagged trees from a bootstrap sample of trainSetD
    #setD = dat[sample(1:nrow(dat), nrow(dat), replace=TRUE),]
    
    # find subsample of var
    # when training the ith tree we only make available a small random subset 
    #subvar = sample(2:ncol(setD), sqrt(ncol(setD)-1), replace = FALSE)
    #subfeat = colnames(setD)[subvar]
    
    adaFit <- ada(lesion_label ~ ., data = dat, #setD[c("lesion_label",subfeat)],
               iter = 400, type = "gentle", nu = 1, bag.shift = TRUE, control=adacontrol)
    #print(adaFit)           
    # append
    forest <- append(forest, list(tree = adaFit))    
  }
  
  output <- list(forest=forest)
  return(output)
}

```


### code forest Test: 
### parameters, T= # of trees, forest, TrainsetD, TestsetD
```{r}

library(pROC)
library(ada)
rpart_adaforestTest <- function(T, TrainsetD, TestsetD, forest) {
  
  fclasspotrain=list()
  for (t in 1:T){
    # Calcultate posterior Probabilities on grid points
    temp <- predict(forest[t]$tree, newdata = TrainsetD, type="prob") #
    fclasspotrain <- append(fclasspotrain, list(cpo = temp))
  }
  
  # run testing cases
  fclasspotest=list()
  for (t in 1:T){
    # Calcultate posterior Probabilities on grid points
    temp <- predict(forest[t]$tree, newdata = TestsetD, type="prob") #
    fclasspotest <- append(fclasspotest, list(cpo = temp))
  }
  
  # performance on Train/Test set separately
  # extract ensamble class probabilities (when T > 1)
  trainpts = fclasspotrain[1]$cpo
  testpts = fclasspotest[1]$cpo
  # init ensample class posteriors
  enclasspotrain <- matrix(, nrow = nrow(as.data.frame(trainpts)), ncol = 2)
  enclasspotest <- matrix(, nrow = nrow(as.data.frame(testpts)), ncol = 2)
  enclasspotrain[,1] = fclasspotrain[1]$cpo[,1]
  enclasspotest[,1] = fclasspotest[1]$cpo[,1]
  enclasspotrain[,2] = fclasspotrain[1]$cpo[,2]
  enclasspotest[,2] = fclasspotest[1]$cpo[,2]
  if(T>=2){
    for (t in 2:T){
      #train
      enclasspotrain[,1] = enclasspotrain[,1]+fclasspotrain[t]$cpo[,1]
      enclasspotrain[,2] = enclasspotrain[,2]+fclasspotrain[t]$cpo[,2]
      #test
      enclasspotest[,1] = enclasspotest[,1]+fclasspotest[t]$cpo[,1]
      enclasspotest[,2] = enclasspotest[,2]+fclasspotest[t]$cpo[,2]
    }
  }
  # majority voting averaging
  enclasspotrain = (1/T)*enclasspotrain
  enclasspotest = (1/T)*enclasspotest
  
  # on training
  classes = levels(TrainsetD$lesion_label)
  trainprob = data.frame(C1=enclasspotrain[,1],
                         C2=enclasspotrain[,2],
                         pred=classes[apply(enclasspotrain, 1, which.max)], 
                         obs=TrainsetD$lesion_label)
  colnames(trainprob)[1:2] <- classes
  pred=as.factor(apply(enclasspotrain, 1, which.max))
  levels(pred) = levels(as.factor(unclass(TrainsetD$lesion_label)))
  perf_train = confusionMatrix(pred, as.factor(unclass(TrainsetD$lesion_label)))
  #print(perf_train)
  
  # on testing
  testprob = data.frame(C1=enclasspotest[,1],
                        C2=enclasspotest[,2],
                        pred=classes[apply(enclasspotest, 1, which.max)], 
                        obs=TestsetD$lesion_label)
  colnames(testprob)[1:2] <- classes
  pred=as.factor(apply(enclasspotest, 1, which.max))
  levels(pred) = levels(as.factor(unclass(TrainsetD$lesion_label)))
  pred=as.factor(apply(enclasspotest, 1, which.max))
  
  groundT = as.factor(unclass(TestsetD$lesion_label))
  levels(groundT) = levels(as.factor(unclass(TrainsetD$lesion_label)))
  groundT= as.factor(unclass(TestsetD$lesion_label))
  
  perf_test = confusionMatrix(pred, groundT)
  #print(perf_test)  
  
  output <- list(etrain = perf_train, etest=perf_test, trainprob=trainprob, testprob=testprob)
  return(output)
}

```


### code for running and plotting perfm results: 
###statsAU
```{r fig.width=12, fig.height=12, message=FALSE, warning=FALSE, width = 200}
# create_ensemble
create_ensemble <- function(dat, particvfoldK, cvK){
  #inint
  ensemblegrdperf=list()
  maxM=list()
  for(r in 1:cvK){
    ## pick one of cvfold for held-out test, train on the rest
    kparti_setdata = kparti_sample(dat, particvfoldK, cvK, r)
    
    # Boruta on $cvTrainsetD
    selfeatures_kfold = subset_select(na.omit(kparti_setdata$cvTrainsetD))
    names(selfeatures_kfold)
    
    ###################################################
    # create grid of evaluation points
    gT = c(1)#c(5,10,30,60,100) 
    gD = c(1,2,5,10,20) 
    grd <- expand.grid(x = gD, y = gT)
    
    ###################################################
    # for oneshot
    grdperf = data.frame(grd)
    grdperf$acuTrain =0
    grdperf$rocTrain =0
    grdperf$senTrain =0
    grdperf$speTrain =0
    
    grdperf$acuTest =0
    grdperf$rocTest =0
    grdperf$senTest =0
    grdperf$speTest =0
    
    M=list()
    for(k in 1:nrow(grd)){
      D=grd[k,1]
      T=grd[k,2]
      # Build in l
      cat("D: ", D, "T: ", T, "\n")
      TrainsetD <-  kparti_setdata$cvTrainsetD[c(names(selfeatures_kfold))]
      TestsetD <-  kparti_setdata$cvTestsetD[c(names(selfeatures_kfold))]
      fit <- rpart_adaforestTrain(T, D, TrainsetD[c(2:ncol(TrainsetD))], TestsetD[c(2:ncol(TestsetD))])
      # # predict
      perf <- rpart_adaforestTest(T, TrainsetD[c(2:ncol(TrainsetD))], TestsetD[c(2:ncol(TestsetD))], fit$forest)
      # for train
      ROCF_train <- roc(perf$trainprob$obs, perf$trainprob$mass, col="#000086", main=paste0("mass ROC T=",T," lrate=",lrate," cv=",r))
      print(ROCF_train$auc)
      # collect data
      grdperf$acuTrain[k] = grdperf$acuTrain[k]+as.numeric(perf$etrain$overall[1])
      grdperf$rocTrain[k] = grdperf$rocTrain[k]+as.numeric(ROCF_train$auc)
      grdperf$senTrain[k] = grdperf$senTrain[k]+as.numeric(perf$etrain$byClass[1])
      grdperf$speTrain[k] = grdperf$speTrain[k]+as.numeric(perf$etrain$byClass[2])
      # for test
      #par(new=TRUE)
      ROCF_test <- roc(perf$testprob$obs, perf$testprob$mass, col="#860000", main=paste0("ROC T=",T," lrate=",lrate," cv=",r))
      #legend("bottomright", legend = c("train", "test"), col = c("#000086", "#860000"),lwd = c(2,1))
      print(ROCF_test$auc)
      # collect data
      grdperf$acuTest[k] = grdperf$acuTest[k]+as.numeric(perf$etest$overall[1])
      grdperf$rocTest[k] = grdperf$rocTest[k]+as.numeric(ROCF_test$auc)
      grdperf$senTest[k] = grdperf$senTest[k]+as.numeric(perf$etest$byClass[1])
      grdperf$speTest[k] = grdperf$speTest[k]+as.numeric(perf$etest$byClass[2])
    
      # append perfm for ROC
      M = append(M, list(M=list(D=D,T=T,trainprob=perf$trainprob,testprob=perf$testprob,forest=fit$forest)))
    }
    print(grdperf)
    index = which(grdperf$rocTest == max(grdperf$rocTest), arr.ind=TRUE)
    Dmax = grdperf$x[index]
    Tmax = grdperf$y[index]
    resamMax = M[index]$M$testprob
    #append
    maxM <- append(maxM, list(maxp=list(D=Dmax,T=Tmax,trainprob=M[index]$M$trainprob,testprob=M[index]$M$testprob,forest=M[index]$M$forest)))
    ensemblegrdperf <- append(ensemblegrdperf, list(grdperf = grdperf))
  }
  
  output <- list(ensemblegrdperf=ensemblegrdperf, maxM=maxM)
  return(output)
}    

surface_forestperfm <- function(grdperf){
  library(gridExtra) 
  library(base)
  library(lattice)
  
  
  graphlist<-list()
  count <- 1
  # design acuTrain
  z = grdperf$acuTrain
  gD=unique(grdperf$x)
  gT=unique(grdperf$y)
  dim(z) <- c(length(gD), length(gT))
  w1 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = "Depth of trees (D)", ylab = "Number of trees (T)",
                  main = "Influence of forest parameters on Accuracy Train",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w1
  count <- count+1
  
  # design rocTrain
  z = grdperf$rocTrain
  dim(z) <- c(length(gD), length(gT))
  w2 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = "Depth of trees (D)", ylab = "Number of trees (T)",
                  main = "Influence of forest parameters on ROC Train",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w2
  count <- count+1
  
  # design acuTest
  z = grdperf$acuTest
  dim(z) <- c(length(gD), length(gT))
  w3 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = "Depth of trees (D)", ylab = "Number of trees (T)",
                  main = "Influence of forest parameters on Accuracy Test",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w3
  count <- count+1
  
  # design rocTest
  z = grdperf$rocTest
  dim(z) <- c(length(gD), length(gT))
  w4 <- wireframe(z, gD,gT,  box = FALSE,
                  xlab = "Depth of trees (D)", ylab = "Number of trees (T)",
                  main = "Influence of forest parameters on ROC Test",
                  drape = TRUE,
                  colorkey = TRUE,
                  light.source = c(10,0,10), 
                  col.regions = colorRampPalette(c("red", "blue"))(100),
                  screen = list(z = 30, x = -60))
  graphlist[[count]]<-w4
  count <- count+1
  
  
  # finally plot in grid
  do.call("grid.arrange",c(graphlist,ncol=2))
}

```

Run for mass lesions:
=====
```{r fig.width=12, fig.height=12, message=FALSE, warning=FALSE, width = 200}
# read mass features
stage1dat = rpart_inputdata(subdata="stage1")
## create CV
cvK = 10
# run for mass
particvfoldK = cvfold_partition(stage1dat, cvK)
res = create_ensemble(stage1dat, particvfoldK, cvK)
accum_stage1grdperf = res$ensemblegrdperf[1]$grdperf
for(k in 2:cvK){
  accum_stage1grdperf = accum_stage1grdperf+res$ensemblegrdperf[k]$grdperf
}
cvKstage1grdperf = accum_stage1grdperf/cvK
print(cvKstage1grdperf)

# plot
surface_forestperfm(cvKstage1grdperf)

# plot ROC of resamples at max perf across cvFolds
resamROC_train=data.frame()
resamROC_test=data.frame()
for(k in 1:cvK){
  resamROC_train = rbind(resamROC_train, res$maxM[k]$maxp$trainprob)
  resamROC_test = rbind(resamROC_test, res$maxM[k]$maxp$testprob)
}
# for resamROC
ROCF_train <- plot.roc(resamROC_train$obs, resamROC_train$mass, col="#000086", lty=1)
par(new=TRUE)
ROCF_test <- plot.roc(resamROC_test$obs, resamROC_test$mass, col="#860000", lty=2,
                      main="boosting ROC for stage1 max cvFolds")
print(ROCF_train$auc)
print(ROCF_test$auc)
legend("bottomright", 
       legend = c(paste0("train: AUC=", formatC(ROCF_train$auc,digits=2, format="f")), 
                  paste0("cv.test: AUC=", formatC(ROCF_test$auc,digits=2, format="f"))), 
       col = c("#000086", "#860000"),lwd = 2, lty = c(1,2))

#save
save.image("Z:/Cristina/MassNonmass/Section1 - ExperimentsUpToDate/experimentsRadiologypaper-revision/Tree-based-RF/ensemble-Treebased-RF/results/cvKstage1grdperfboosting.RData")

```
